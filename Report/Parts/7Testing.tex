\chapter{Testing}
\label{Testing}
In this chapter we discuss our testing strategy, our results using said strategy and what we feel we could have done to improve our testing overall.
\section{Strategy}
\label{Testing_Strategy}
Our testing strategy consists of different kinds of tests and a tight integration with how user stories work in the Scrum development strategy we are using. We have three different kinds of software tests: Scenario tests, Service tests and Graphical User Interface (GUI) tests. We also have usability tests to ensure we have decent interface in terms of usability.

Almost all functionality in our RentIt system is implemented by user stories. A user story is not accepted until tests of the functionality pass and the tests have been reviewed by the team member responsible for QA. The team member responsible for QA is Jakob Melnyk. If he is the one who created the test, a different teammember takes care of the QA for that test.

To us, this ensures an acceptable level of peer review during development of the system. Between the feature freeze and code freeze dates, everyone reviews both the code and the tests, so that we get a more thorough peer review.

\subsection{Code test types}
\label{Testing_Strategy_Types}
We have different testing categories for testing different levels of the code of the system. The different levels we have defined are scenario, service and end-user levels. This section describes the kind of tests done at the different levels. We originally had method/unit testing as well, but we decided the way we have structured our system does not make it easy to do standard unit testing.

\begin{my_description}
\item[Scenario-level] Mainly tests designed to cover a specific user story.
\item[Service-level] Mainly consist of testing the connection to the service and the different service contracts.
\item[End-User-level] Test functionality of the GUI for the end-user.
\end{my_description}

\subsubsection{Scenario-level tests}
\label{Testing_Strategy_Types_Scenario}
We have used scenario tests to test features in our project. Features such as editing a user profile or renting a movie are tested on the classes containing the logic for editing the database and filtering information. This is done to seperate the logic from the Service class itself, such that the service can be exchanged without affecting the logic much (if at all). This also enables us to test the logic seperately from the service itself.

Most of our automated tests are scenario tests\footnote{Our automated tests number 66 scenario tests, 31 service tests and 13 GUI tests.}. We have chosen to put our focus on scenario tests, because scenarios are integral to the way, we have developed our service. Most features are implemented by making one or two methods to cover the necessary implementation of the feature \footnote{Design and architecture is more thoroughly covered in chapter \ref{Design} \nameref{Design}.}. Our service test list (with results) can be found in the appendix on page \pageref{Appendix_Test_Table_Scenario}.
\subsubsection{Service-level tests}
\label{Testing_Strategy_Types_Service}
Our service levels tests do not cover as much of the API as our scenario tests and are not as thorough in testing the functionality. They are mostly intended to test for connection problems, bindings\footnote{Covered briefly in chapter \ref{Design} \nameref{Design}.} issues, data contracts and error handling. Our service test list (with results) can be found in the appendix on page \pageref{Appendix_Test_Table_Service}.

\subsubsection{Graphical user interface tests}
\label{Testing_Strategy_Types_EndUser}
Testing that the service works is not the only important thing to do. It is also important to make sure that the graphical user interface works,  otherwise the user will not be able to use the service for anything. We split the testing of the graphical user interface into two parts: Automatic tests and manual tests. They are described below. 

\paragraph{Automatic GUI tests}
The automatic GUI tests have been made through the use of the Coded UI Tests in Visual Studio 2010. When we create such a test, we record the actions we make in the client through a program, which then saves the workflows. When a test has been recorded, it can be run any amount of times and it will take the same actions every time. Using this we made automatic tests for all the basic features, such as signing in, logging in, searching for movies, etc. They are listed in the appendix under \ref{Appendix_Test_Table_GUI} on page \pageref{Appendix_Test_Table_GUI}.

\paragraph{Manual GUI test}
While we could automate some tests, there were others that were not worth automating. Automating things like upload/download would be too hard, as file directories change from computer to computer. This meant that we had to test certain functionalities manually, simply by opening the client and going through the necessary steps. They are listed in the appendix under \ref{Appendix_Test_Manual} on page \pageref{Appendix_Test_Manual}.

\subsection{Regression tests}
\label{Testing_Strategy_Regression}
Every time we run in to an error or a bug in the system, our strategy is to create a test that covers the scenario the error presented itself in. The test is intended to fail the first time it is run (to verify the bug exists), then when the issue has been fixed, the test passes. 
\\These tests are only meant to cover the specific bug they were designed to, but because they are already designed (to check if the bug has been fixed), they are kept as regression tests to ensure the bug or error is not reintroduced later.
\\All of our automated tests can serve as regression tests, as they do the same thing - ensure that functionality is not broken by later changes.

\subsection{Usability tests}
\label{Testing_Strategy_Usability}
When designing a user interface you have to take into account that not all users is equally proficient in navigating IT systems therefore we have to design a interface which is easy to use. To accomplish this we did a couple of usability tests. Usability tests is a testing technique which focuses on the usability of a user interface, this is measured in non-functional requirements. For usability testing you need a mockup to test against, you then make a list of usability goals
\footnote{See \ref{Appendix_Test_Usability_GoalsandScenario} for our usability goals} if these goals is fulfilled then you have the user interface that you wanted. For the usability test itself you make a list of scenarios that the users is asked to go through \footnote{See \ref{Appendix_Test_Usability_GoalsandScenario} for our usability scenarios}. While performing the scenarios, the user is asked to think aloud, such that the overseer of the test can take notes on how to improve the system.

The way we went about doing our usability tests, was to first set down as a team and create some paper mockups, which we found user friendly and had high ease-of-use. We then made some usability goals which if fulfilled, would ensure us that our interfaced was indeed user friendly and had a high ease-of-use. With these we made our first usability test on the paper mockups.We had two test users go through our usability scenarios, when they where done we then assessed how they compared to the usability goals.

For the second usability test we created a digital version of our paper mockups, but this time we added some dialog and confirmationboxses to ensure that the user didn't feel that their changes would go unsaved. Besides that we change abit of the design but without deviating to much from the paper mockups. We then conducted the test the same way as we did with the first, but this time on the digital version.

\subsection{Code coverage}
\label{Testing_Strategy_Coverage}
It is rarely an effective strategy to cover every combination of paths through the code, because it is very time-consuming (both in creating tests and actually running those tests). Instead it is important to test enough to cover a lot of code and functionaliy without crossing the line of where it starts being redundant to do so\cite{WoT}. With that in mind, we have decided to use a twofold approach to our tests. We write tests to cover different inputs and scenarios for our features, then assess our code coverage. If the coverage has not reached our goal, we think up new tests to increase the coverage procentages.

Our requirements for code coverage are as follows:
\begin{my_itemize}
\item Minimum overall coverage is 50\% across solution.
\item Goal is to have 80\% overall coverage of client and service.
\end{my_itemize}

\paragraph{Minimum overall}
While a 50\% code coverage may seem like a low number, we use this number because the critical parts of our system do not take up a big percentage of the code. This is due to the GUI elements, the Model-View-ViewModel architecture of the client and some elements in the service. In addition, the 50\% requirement is only an absolute minimum and not necessarily a number we will be close to in the end.

\paragraph{Overall goal}
The overall coverage percentage goal is quite different from our minimum requirement. This is because, in an optimal scenario, we will be able to test the MVVM architecture and the GUI parts (and run code coverage) on the tests we write for those parts. The best code coverage percentage is of course 100\%. However, seeing as our system is not "mission critical"\footnote{Mission critical in this case refers to loss of life and/or loss of money.} as we have not (yet) implemented actual payment options, we feel that 80\% is a good number based on previous experiences.

\paragraph{Workflows}
\label{Testing_Strategy_Coverage_Workflows}
In addtion to covering functionality on the service, we also need to cover "workflows" in the client. The easiest way for us to check if we have fulfilled the requirements of the project is to simply do a workflow that incorporates the use cases described in \ref{Requirements_UC} on page \pageref{Requirements_UC}.Our list in the appendix on page \pageref{Appendix_Test_Manual} describes which use cases are covered by which manual test.

\subsection{Configurations}
\label{Testing_Strategy_Configurations}
We use a different service and database for running our tests. The aim is to have a working version of the service on both the production/release address as well as the test address. Having seperate databases lets us reset data on the test database without having to fiddle with data on the production service.
\\The bad thing about doing this is that we have to make sure that the two versions are kept up to date. Else we cannot be certain about the quality of the service from looking at the tests.

\section{Test results}
\label{Testing_Results}
We do not have 100\% test pass in our automatic tests. 
\subsection{Code coverage}
\label{Testing_Results_Coverage}
\begin{my_itemize}
	\item Service\footnote{Full coverage table can be found in section \ref{Appendix_Test_Coverage} on page \pageref{Appendix_Test_Coverage}.} | 58\%
	\begin{my_itemize}
	\item RentItService.Services | 0\%
	\item RentItService.Exceptions | 17\%
	\item RentItService.Library | 61\%
	\item RentItService.Entities | 75\%
	\item RentItService | 100\%
	\item RentItService.Mapping| 100\%
	\end{my_itemize}
\end{my_itemize}

All in all we feel our values are quite acceptable.A coverage of 0\% in the \class{Services} namespace was to be expected, as we did not design scenario tests for that namespace. We could have done to increase code coverage, but because we also do service-level tests, we felt we should put our focus elsewhere.
\\ All the exceptions inherit from the base \class{Exception} class and as such have constructors we don't use. We could artificially pump up the coverage of the \class{Exceptions} namespace by making tests for it, but we felt it unnecessary. The library has a low 61\% because we do not test upload and/or download in the scenario tests. Because we have some bugs in both editions and rental, we have low coverage in \class{Entities}.

Our goal was a 80\% overall coverage of the service, which we are a bit ways away from achieving, but the numbers could be reached through testing of the service code and fixing the problems in Edition and Rental.
\subsection{Results of GUI workflows}
\label{Testing_Results_Workflows}
Some of our automatic tests for the GUI workflows fail because of weird bug outs in the framework we use. The ones that fail (and a couple of non automated GUI tests) have been done manually. During testing we found that some of our use cases (and thus requirements, see \ref{Requirements_UC} on page \pageref{Requirements_UC}) are not fulfilled as some of our tests fail. \\The requirements we do not cover with successful tests of the use cases are: "User - Download media", "Content Provider - Upload Media" \&  "Content Provider - Edit Media".
\section{Reflection on test strategy}
\label{Testing_Reflection}
We are pretty happy with the way our strategy turned out. It forces us to develop our service in modules and forces us to make our client able to perform the use cases from our requirements. Because we designed tests quite early (tests were made for each user story every time it was finished), we were forced to change them when we made major changes to the service logic during refactoring. This was both good and bad, as we could not use our code coverage tools for a while.
\subsection{Improving our testing strategy}
\label{Testing_Reflection_improvements}
In order to cover more code with tests, we could have use an automated unit test generation framework such as Pex\cite{PEX}. To really use frameworks such as Pex to their full potential, we would need to design our system in such a way that the logic of the service is not only seperated from the classes representing the endpoints (like we have done), but also seperated the database communication logic from the service logic. We could also "artificially" create more hand-written tests to cover the cases described in \ref{Testing_Results_Coverage}.

We should probably have defined some clearly critical sections for testing (such as service logic enabling the core requirements). This would have made us focus on making specific features work first before designing more elaborate ones.
