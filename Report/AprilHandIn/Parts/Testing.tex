\chapter{Testing}
\label{testing}
In this chapter we discuss our testing strategy, our results using said strategy and what we feel we could have done to improve our testing overall. 

\section{Strategy}
\label{testing_strategy}
Our testing strategy consists of different kinds of tests and a tight integration with how our user stories work in Scrum development strategy. We have four different kinds of tests: Automaticaly generated tests, Scenario tests, Service tests and GUI tests. 

Almost all functionality in our RentIt system is implemented by user stories. A user story is not accepted until tests of the functionality pass and the tests have been reviewed by the team member responsible for QA. The team member responsible for testing is Jakob Melnyk, unless he is the one who has made the tests. 

To us, this ensures an acceptable level of peer review during development of the system. Between the feature freeze and code freeze dates, everyone reviews both the code and the tests, so that we get a more thorough peer review.

\subsection{Test types}
\label{testing_strategy_types}
We have different testing categories for testing different levels of the system. The different levels we have defined are scenario, service and end-user levels. This section describes the different levels we use for testing. We originally had method/unit testing as well, but we decided the way we have structured our systems do not make it easy to do.

\begin{description}
\item[Scenario-level]Mainly tests designed to cover a specific user story.
\item[Service-level]Mainly consist of testing the connection to the service and the different service contracts.
\item[End-User-level]Test functionality of the GUI for the end-user.
\end{description}

\subsubsection{Scenario-level tests}
\label{testing_strategy_types_scenario}
We have used scenario tests to test features in our project. Features such as editing a user profile or renting a movie are tested on the classes containing the logic for editing the database, creating filestreams and filtering information. This is done to seperate the logic from the Service class itself, such that we do not have to start or contact the service everytime we need to test.

Most of our tests are scenario tests. We have chosen to put our focus on scenario tests, because scenarios are integral to the way we have designed our service. Most features are implemented by making one or two methods to cover the necessary implementation of the feature \footnote{Design and architecture is more thoroughly covered in our Design chaper}.

\subsubsection{Service-level tests}
\label{testing_strategy_types_service}


\subsubsection{Graphical interface tests}
\label{testing_strategy_types_enduser}


\subsection{Regression tests}
\label{testing_strategy_regression}


\subsection{Code coverage}
\label{testing_strategy_coverage}
It is rarely an effective strategy to cover every combination of paths through the code, because it is very time-consuming (both in creating tests and actually running those tests). Instead it is important to test enough to cover a lot of your code and functionaliy without crossing the line of where it is redundant to do so\cite{The Way of Testivus}. With that in mind, we have decided to use twofold approach to our tests. We write tests to cover different inputs and scenarios for our features, then assess our code coverage. If the coverage has not reached our goal, we think up new tests to increase the coverage procentages.

Our requirements for code coverage are as follows:
\begin{itemize}
\item Minimum overall coverage is 50\% across solution.
\item Goal is to have 85\% overall coverage of client and service.
\item Minimum coverage of critical sections is 90\%.
\end{itemize}

\paragraph{Minimum overall}
While a 50\% code coverage may seem like a low number, we use this number because the critical parts of our system do not take up a big percentage of the code. This is due to the GUI elements, the Model-View-ViewModel\footnote{Discussed in the Design chapter} architecture of the client and some elemnts in the service. In addition, the 50\% requirement is only an absolute minimum and not necessarily a number we will be close to in the end.

\paragraph{Overall goal}
The overall coverage percentage goal is quite different from our minimum requirement. This is because, in an optimal scenario, we will be able to test the MVVM architecture and the GUI parts (and run code coverage) on the tests we write for those parts. The best code coverage percentage is of course 100\%, but seeing as our system is not "mission critical"\footnote{Mission critical in this case refers to loss of life and/or loss of money} as we have not (yet) implemented actual payment options, we feel that  	

\paragraph{Critical sections}


The sections in the code that we consider critical are the following:
\begin{itemize}
\item
\item
\end{itemize}

Why we feel these parts of the code are critical (and why we feel the rest is not as critical) is explained in greater detail in our Design chapter.

\section{Test results}
\label{testing_results}

\subsection{Code coverage}
\label{testing_results_coverage}

\section{Reflection on test strategy}
\label{testing_reflection}