\chapter{Testing}
\label{testing}
In this chapter we discuss our testing strategy, our results using said strategy and what we feel we could have done to improve our testing overall. 

\section{Strategy}
\label{testing_strategy}
Our testing strategy consists of different kinds of tests and a tight integration with how our user stories work in Scrum development strategy. We have four different kinds of tests: Automaticaly generated tests, Scenario tests, Service tests and GUI tests. 

Almost all functionality in our RentIt system is implemented by user stories. A user story is not accepted until tests of the functionality pass and the tests have been reviewed by the team member responsible for QA. The team member responsible for testing is Jakob Melnyk, unless he is the one who has made the tests. To us, this ensures an acceptable level of peer review during development of the system. Between the feature freeze and code freeze dates, everyone reviews both the code and the tests, so that we get a more thorough peer review.

\subsection{Test types}
\label{testing_strategy_types}
We have different testing categories for testing different levels of the system. The different levels we have defined are method, scenario, service and end-user levels. This section describes the different levels we use for testing.

\begin{description}
\item[Method-level] Involves testing a method in no context outside of just running the code. 
\item[Scenario-level] Scenario tests are mainly scenarios described in a user story or something a user "wants to do".
\item[Service-level] Mainly consist of testing the connection to the service and the different service contracts.
\item[End-User-level] Test functionality of the GUI for the end-user.
\end{description}

\subsubsection{Method-level tests}
\label{testing_strategy_types_method}
Our method-level testing are all whitebox tests generated by Pex. These mainly serve to check if contracts work and to cover code in terms of code "being run". These are not very effective because of the way we write contracts. We have few ensures, but quite a few requires. This means Pex can detect when a method should fail, but because the methods do not ensure anything, it cannot check that thoroughly.

\subsubsection{Scenario-level tests}
\label{testing_strategy_types_scenario]
We have used scenario tests to test features in our project. Features such as editing a user profile or renting a movie are tested on the classes containing the logic for editing the database, creating filestreams and filtering information. This is done to seperate the logic from the Service class itself, such that we do not have to start or contact the service everytime we need to test.

Most of our tests are scenario tests. We chose to put our focus on the scenario/feature tests, because they are so integral to our design. Almost every feature is done as single operations and simple steps.

\subsubsection{Service-level tests}
\label{testing_strategy_types_service}

\subsubsection{End-user tests}
\label{testing_strategy_types_enduser}

\subsection{Code coverage}
\label{testing_strategy_coverage}

\subsubsection{Critical code sections}
\label{testing_strategy_coverage_critical}
As with most software, not all parts of the code is as critical as the rest. We have decided to split our code into two sections: Critical and non-critical.

The sections in the code that we consider critical are the following:
\begin{itemize}
\item
\item
\end{itemize}

We explain why we feel these sections are critical in another chapter of the report.

\section{Test results}
\label{testing_results}

\subsection{Code coverage}
\label{testing_results_coverage}
Minimum 50% coverage
Aim for 95% coverage in 'critical' sections.
Impossible to get 100% in whole solution, ideal would be 100% in critical and 80% in non-critical.
\section{Reflection on test strategy}
\label{testing_reflection}